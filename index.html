<!DOCTYPE HTML>

<html>
	<head>
		<title> </title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="css/ie/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/jquery.dropotron.min.js"></script>
		<script src="js/jquery.scrollgress.min.js"></script>
		<script src="js/jquery.scrolly.min.js"></script>
		<script src="js/jquery.slidertron.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
			<link rel="stylesheet" href="css/style-xlarge.css" />
		</noscript>
		<!--[if lte IE 9]><link rel="stylesheet" href="css/ie/v9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="css/ie/v8.css" /><![endif]-->
		<script type="text/x-mathjax-config">
  			MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
		<script type="text/javascript"
			src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-135826741-1"></script>
	    <script>
	      window.dataLayer = window.dataLayer || [];
	      function gtag(){dataLayer.push(arguments);}
	      gtag('js', new Date());
   
     	  gtag('config', 'UA-135826741-1');
	    	</script>

	</head>
	<body class="landing">

		<!-- Header -->
			<header id="header" class="alt skel-layers-fixed">
				<nav id="nav">
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="simulations.html">Simulations</a></li>
						<li><a href="resources.html">Resources</a></li>
						<li><a href="things/CV.pdf", target="_blank">CV</a></li>
						<li><a href="contact.html">Contact</a></li>
					</ul>
				</nav>
			</header>

		<!-- Banner -->
			<section id="banner">
				<div class="inner">
					<h2>Hello!</h2>
					<p>I'm Chris Pedersen, a deep learning researcher and astrophysicist working at New York University and the Flatiron Institute</p>
					<ul class="actions">
						<li><a href="#one" class="button big scrolly">Learn More</a></li>
					</ul>
				</div>
			</section>

		<!-- One -->
			<section id="one" class="wrapper style1">
				<div class="container">
					<header class="major">
						<p><div align="left"><span class="image left"><img src="images/myface.jpg" alt="" width="200" height="200"/></span>
							<span class="image right"><img src="images/max.jpg" alt="" width="200" height="200"/></span>Welcome to my web page! I am a Postdoctoral Associate at the <a href="https://cims.nyu.edu/dynamic/">Courant Institute of Mathematical Sciences</a> and the <a href="https://cds.nyu.edu/">Centre for Data Science</a> at 	New York University, and a Guest Researcher at the <a href="https://www.simonsfoundation.org/flatiron/">Flatiron Institute</a>. I am interested in the application of novel methods in machine learning (ML) to solve problems across science. This is a two-pronged area of research. On the one hand, the rapid development of new ML techniques has enabled new approaches to tackle a vast number of outstanding scientific problems. Additionally, the complex and challenging nature of these problems can serve to drive innovation in the ML space.
							
						<br><br>

						In this vein, I am working with <a href="https://laurezanna.github.io/">Laure Zanna</a>, <a href="https://cims.nyu.edu/~bruna/">Joan Bruna</a>, and <a href="https://math.nyu.edu/~cfgranda/">Carlos Fernandez-Granda</a> in the <a href="https://m2lines.github.io/"">M2LInES</a> collaboration working on using ML to improve the robustness of climate modelling. Previously, I was a postdoc at NYU's <a href="https://cosmo.nyu.edu/">Centre for Cosmology and Particle Physics</a>, working with <a href="https://users.flatironinstitute.org/~sho/index.html">Shirley Ho</a> on ML applications across science. Before that, I completed my PhD in astrophysics at the <a href="https://www.ucl.ac.uk/cosmoparticle/">Cosmoparticle Initiative</a> at University College London, after obtaining my Masters in Physics with Astronomy from Cardiff University.<br><br>

						Please find below a brief summary of some of my previous and ongoing research projects. A formal CV can be found <a href="things/CV.pdf">here</a>, and my contact information <a href="contact.html">here</a>.
						
				</div>
				<br>
			</section>



			<section id="content">
				<div class="container">
					<section id="content">

						<section>
							<br>
							<header class="major">
								<h3 style="text-align:left">AI-driven equation discovery for turbulence modelling</h3>
								<p><div align="left"><span class="image right"><img src="images/project_images/turbulence_wiki.jpg" alt="" width="200" height="200"/></span>
									Modelling the evolution of Earth's climate requires detailed simulations of the oceans and atmosphere. These simulations are extremely computationally expensive, and we are limited by computational resources in the small-scale dynamics that one can resolve. However, turbulence is a notoriously multi-scale phenomena, with small-scale eddies and vorticies affecting the large scale evolution of fluids. Therefore ocean simulations of finite resolution are missing these important effects.

									In this project I am working on using novel machine learning techniques to discover new systems of equations that will enable low-resolution (and therefore computationally tractable) ocean simulations to include these small-scale turbulent effects.
								
									<br><br>
									<i>Project ongoing..</i>
								</div></p>
						</section>
						
						<hr />

						<section>
							<header class="major">
								<h3 style="text-align:right">Cancer-net</h3>
								<p><div align="left"><span class="image left"><img src="images/project_images/graph.png" alt="" width="200" height="200"/></span>
									The cost of DNA sequencing has reduced by orders of magnitude over the past few years, enabling the possibility of modelling the progression of cancer tumors as a function of their genetic mutations as more data becomes available. However there are still a unique set of challenges, as this highly protected data is segregated into small groups, making it difficult to train models on long genetic sequences from only a few hundreds of patients.
									<br><br>
									By grouping genes into subgraphs using biological prior knowledge, and using sparse connections between these graphs, we are building a binary classification model of prostate cancer tumors, which will predict whether or not a tumor is metaststic based on it's genetic markers. The motivation behind the graph network is to enable increased generalisability, and the sparse connections reduce the number of parameters of the model, enabling training on the small datasets available.
									<br><br>
									<i>Project ongoing..</i>
								
								</div></p>
						</section>
					

						<hr />

						<section>
							<header class="major">
								<h3 style="text-align:left">Learnable wavelet neural networks for cosmological inference</h3>
								<p><div align="left"><span class="image right"><img src="images/project_images/wavelet.png" alt="" width="200" height="200"/></span>
								An important part of cosmology is the process of extracting information from the cosmological fields observed by experiments and telescopes. Traditionally, these fields are compressed into summary statistics (the two-point correlation function), and then compared with theory to obtain constraints on physical parameters. However it is known there is information lost in this process. Recent work has shown that convolutional neural networks (CNNs) are able to retain this information, while optimally marginalising over astrophysical nuisance parameters.
								<br><br>
								CNNs however require notoriously large amounts of data to train, and in the case of cosmological analysis, have to be trained on expensive numerical simulations. Given the limitations of computational resources, there is a strong motivation for designing models that can be trained on smaller amounts of training data. We designed a wavelet-based convolutional neural network, where the first 2 convolutional layers use wavelets instead of the traditional grid filters. The input fields are downsampled by factor of 2 after each convolution, meaning the wavelet layers are effectively performing an efficient compression of the data. The output of the wavelet convolutional layers are passed to a standard CNN of only 3 layers.
								<br><br>
								This model has an order of magnitude less learnable parameters than a standard CNN, and we show it dramatically outperforms a CNN at inference of cosmological parameters when in the regime of small training set sizes. Additionally, the wavelet filter parameters are included in the gradient descent, and can be used to understand where the information lies in the cosmological fields.
								<br><br>
								<i><a href="https://ml4astro.github.io/icml2022/assets/40.pdf">Published at ICML 2022 Machine learning for astronomy workshop</a></i>

						</section>


						<hr />

						<section>
								<header class="major">
									<h3 style="text-align:right">Compressing cosmological information</h3>
									<p><div align="left"><span class="image left"><img src="images/project_images/Lymanalpha.gif" alt="" width="200" height="200"/></span>The Lyman-alpha forest, a series of absorption features in the spectra of distant quasars, can be used to measure the growth of structure in the early Universe, and on relatively small scales. This regime is particularly significant in constraints on two active areas of cosmological research. One is the effect of neutrino mass on cosmology, where subtle measurements of the growth of structure can be used to determine the mass of the lightest particle in physics. Another is the scale-dependence of the quantum fluctuations seeded at the Big Bang, which can be used to constrain models of fundamental physics.
									<br><br>
									Whilst this information is extremely valuable, analysis of the Lyman-alpha forest is a highly complicated task, requiring expensive hydrodynamical simulations, and marginalisation over uncertain astrophysical effects which the absorption features we observe also depend on. Only a small number of groups have the expertise and scope required to perform this analysis. As a result of these obstacles, results from the Lyman-alpha forest are underutilised in the joint analysis of cosmological observations. In this work, we tackle this problem by showing that the cosmological information in the Lyman-alpha forest can be compressed into marginalised constraints on just two parameters, in a model-independent way and with negligible loss of information. This will enable groups to include these valuable measurements in their analysis, without having to runn their own simulations or perform their own marginalisation, and will dramatically boost the scientific impact of current and future Lyman-alpha forest observations.
									<br><br>
									<i>Submitted to JCAP, under review.</i>
						</section>

						<hr />

						<section>
								<header class="major">
									<h3 style="text-align:left">Gaussian process emulator</h3>
									<p><div align="left"><span class="image right"><img src="images/project_images/gaussprocess.png" alt="" width="200" height="200"/></span>The Lyman-alpha forest is a valuble probe of the growth of structure in the early Universe, and on small scales. Interpeting observations of the Lyman-alpha forest requires running expensive hydrodynamical simulations, to compare different theoretical models to data. Modern statistical techniques require hundreds of thousands of likelihood evaluations in order to obtain robust statistical constraints on parameters, but we are limited by computational resources to just tens of simulations.

									In this paper, we tackle this problem by running a Latin hypercube of simulations, and use them to train a Gaussian process to predict the Lyman-alpha forest observables throughout parameter space. In particular, we use a physically motivated, lower-dimensional parameter space than previous work, and demonstrate the suitability of this parameter space by accurately predicting the Lyman-alpha forest for models not included in the original training data. This emulator is therefore appropriate for the analysis of several different interesting models simulatenously.
									<br><br>
									<i><a href="https://iopscience.iop.org/article/10.1088/1475-7516/2021/05/033">Published in JCAP</a></i><br>
									<i><a href="https://github.com/igmhub/LaCE/">Gaussian process code</a></i>
						</section>

						<hr />

						<section>
							<header class="major">
								<h3 style="text-align:right">Neutrino mass degeneracy</h3>
								<p><div align="left"><span class="image left"><img src="images/project_images/neutrino.jpg" alt="" width="200" height="200"/></span>One of the most significant contributions of the Lyman-alpha forest to cosmology is it's sensitity to the clustering of matter on small scales. This is particularly relevant in the case of neutrino mass, which causes a scale-dependent suppression of the growth of structure. In this work we run a set of hydrodynamical simulations with the intention to carefully understand the effects of neutrino mass on the forest. Specifically, we want to understand to what extent the effects of neutrino mass are degenerate with an overall rescaling of the amplitude of clustering. The motivation is that it is important to understand parameter degeneracies when designing methods to interpret a given dataset.
								<br><br>
								We find that the Lyman-alpha forest alone is unable to distinguish between the effects of neutrino mass and a uniform rescaling of the clustering amplitude. This has strong implications for the design of future modelling frameworks and likelihoods.
								<br><br>
								<i><a href="https://iopscience.iop.org/article/10.1088/1475-7516/2020/04/025">Published in JCAP</a></i><br>
						</section>

						<hr />

						<section>
							<header class="major">
								<h3 style="text-align:left">Precessing binary black holes</h3>
								<p><div align="left"><span class="image right"><img src="images/project_images/bbh.png" alt="" width="200" height="200"/></span>In 2016, LIGO made the first ever detection of gravitational waves, from a system of two orbiting black holes colliding and merging into one. This opened up a new way to observe the Universe. Recovering the properties of the progenitor black holes of a merger event is crucial in maximising the science that can be done with gravitational wave astronomy. One property that is particularly relevant for understanding the formation mechanism of these binary systems is the spin of the black holes. In systems where the spins of the component black holes are not aligned, the orbital plane of the binary precesses, like a plate or a coin wobbles when dropped on a flat surface.
								<br><br>
								In this project I ran simulated mock merger events, and used Bayesian inference and Markov chain Monte Carlo analysis in order to understand our ability to observe this precession. I found that this strongly depends on the inclination of the binary system with respect to Earth, as well as it's location in the sky, due to parameter degeneracies.
								<br><br>
								<i><a href="https://iopscience.iop.org/article/10.1088/1475-7516/2020/04/025">Masters thesis</a> - seed project for a publication in <a href="https://journals.aps.org/prd/abstract/10.1103/PhysRevD.103.124023">Physical Review  D</a></i><br><br>
						</section>
					</section>
				</section>
			</div>

		<!-- Three -->
			<section id="three" class="wrapper style1">
				<div class="container">
					<header class="major">
						<h3>Research Summary</h3>
						<p><div align="left">I completed my undergraduate and masters degrees at Cardiff University, where my thesis work was in the <a href="https://www.ligo.org/">LIGO collaboration</a> working on the parameter estimation and degeneracies in the waveforms of binary black hole mergers. I obtained my PhD in 2021 from the <a href="https://www.ucl.ac.uk/cosmoparticle/"> Cosmoparticle Initiative</a> at University College London supervised by <a href="https://andreufont.github.io/">Andreu Font-Ribera</a> and <a href="http://thomaskitching.net/">Tom Kitching</a>. The focus of my research was in using hydrodynamical simulations and machine learning in the interpretation of observations of quasar spectra, to be used in constraints on cosmology and neutrino mass. A byproduct of these simulations are some beautiful visualisations of the dark matter density field and cosmic web, such as the above image. I am also interested in neutrino cosmology, gravitational wave astronomy, and the application of machine and deep learning techniques to extract cosmological information from data.</div></p>
					</header>
					<div class="row">
						<div class="4u 6u(2) 12u$(3)">
							<article class="box post">
								<a href="#" class="image fit"><img src="images/pic01.jpg" alt="" /></a>
								<h3>Simulations</h3>
								<p>The high performance supercomputer code that we use to model the universe.</p>
								<ul class="actions">
									<li><a href="simulations.html" class="button">Learn More</a></li>
								</ul>
							</article>
						</div>
						<div class="4u 6u$(2) 12u$(3)">
							<article class="box post">
								<a href="#" class="image fit"><img src="images/pic02.jpg" alt="" /></a>
								<h3>Resources</h3>
								<p>Some things that I like and find interesting.</p>
								<ul class="actions">
									<li><a href="resources.html" class="button">Learn More</a></li>
								</ul>
							</article>
						</div>
						<div class="4u$ 6u(2) 12u$(3)">
							<article class="box post">
								<a href="#" class="image fit"><img src="images/pic03.jpg" alt="" /></a>
								<h3>Contact</h3>
								<p>Email, github and other contact information.</p>
								<ul class="actions">
									<li><a href="contact.html" class="button">Learn More</a></li>
								</ul>
							</article>
						</div>
					</div>
				</div>
			</section>
			
		<!-- Footer -->
			<footer id="footer">
				<ul class="menu">
					<li><a href="contact.html">Contact</a></li>
				</ul>
			</footer>

	</body>
</html>
