<!DOCTYPE HTML>

<html>
	<head>
		<title> </title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="css/ie/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/jquery.dropotron.min.js"></script>
		<script src="js/jquery.scrollgress.min.js"></script>
		<script src="js/jquery.scrolly.min.js"></script>
		<script src="js/jquery.slidertron.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
			<link rel="stylesheet" href="css/style-xlarge.css" />
		</noscript>
		<!--[if lte IE 9]><link rel="stylesheet" href="css/ie/v9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="css/ie/v8.css" /><![endif]-->
		<script type="text/x-mathjax-config">
  			MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
		<script type="text/javascript"
			src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-135826741-1"></script>
	    <script>
	      window.dataLayer = window.dataLayer || [];
	      function gtag(){dataLayer.push(arguments);}
	      gtag('js', new Date());
   
     	  gtag('config', 'UA-135826741-1');
	    	</script>

	</head>
	<body class="landing">

		<!-- Header -->
			<header id="header" class="alt skel-layers-fixed">
				<nav id="nav">
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="simulations.html">Simulations</a></li>
						<li><a href="resources.html">Resources</a></li>
						<li><a href="things/CV.pdf", target="_blank">CV</a></li>
						<li><a href="contact.html">Contact</a></li>
					</ul>
				</nav>
			</header>

		<!-- Banner -->
			<section id="banner">
				<div class="inner">
					<h2>Hello!</h2>
					<p>I'm Chris Pedersen, a deep learning researcher and astrophysicist working at New York University and the Flatiron Institute</p>
					<ul class="actions">
						<li><a href="#one" class="button small scrolly">Bio</a></li>
						<li><a href="#content" class="button small scrolly">Research</a></li>
						<li><a href="#skills" class="button small scrolly">Technical skills</a></li>
					</ul>
				</div>
			</section>

		<!-- One -->
			<section id="one" class="wrapper style1">
				<div class="container">
					<header class="major">
						<p><div align="left"><span class="image leftsmall"><img src="images/myface.jpg" alt="500"/></span>
							<span class="image rightsmall"><img src="images/max.jpg" alt=""/></span>Welcome to my web page! I am a Postdoctoral Associate at the <a href="https://cims.nyu.edu/dynamic/">Courant Institute of Mathematical Sciences</a> and the <a href="https://cds.nyu.edu/">Centre for Data Science</a> at 	New York University, and a Guest Researcher at the <a href="https://www.simonsfoundation.org/flatiron/">Flatiron Institute</a>. I am interested in the application of novel methods in machine learning (ML) to solve problems across science. This is a two-pronged area of research. On the one hand, the rapid development of new ML techniques has enabled new approaches to tackle a vast number of outstanding scientific problems. Additionally, the complex and challenging nature of these problems can serve to drive innovation in the ML space.
							
						<br><br>

						In this vein, I am working with <a href="https://laurezanna.github.io/">Laure Zanna</a>, <a href="https://cims.nyu.edu/~bruna/">Joan Bruna</a>, and <a href="https://math.nyu.edu/~cfgranda/">Carlos Fernandez-Granda</a> in the <a href="https://m2lines.github.io/"">M2LInES</a> collaboration working on using ML to improve the robustness of climate modelling. Previously, I was a postdoc at NYU's <a href="https://cosmo.nyu.edu/">Centre for Cosmology and Particle Physics</a>, working with <a href="https://users.flatironinstitute.org/~sho/index.html">Shirley Ho</a> on ML methods to extract information from cosmological and genomics datasets. Before that, I completed my PhD in astrophysics at the <a href="https://www.ucl.ac.uk/cosmoparticle/">Cosmoparticle Initiative</a> at University College London, after obtaining my Masters in Physics with Astronomy from Cardiff University, where my thesis work was done as part of the <a href="https://www.ligo.org/">LIGO</a> collaboration.<br><br>

						Please find below a brief summary of some of my previous and ongoing research projects. A formal CV can be found <a href="things/CV.pdf">here</a>, and my contact information <a href="contact.html">here</a>.
						
				</div>
				<br>
			</section>

			<section id="content">
				<div class="container">
					<section id="content">
						<section>
							<br>
							<header class="major">
								<h3 style="text-align:right">Thermalizer: Stable neural emulation of spatiotemporal chaos</h3>
								<p><div align="left"><span class="image leftsmall"><img src="images/project_images/kol_highres.png" alt=""/></span>
									Chaotic dynamical systems are ubiquitous throughout science and engineering, but computationally expensive to model using numerical methods. Deep learning has been used to create fast, approximate models of chaotic systems in the form of autoregressive models, however such models are notoriously unstable over long timescales due to accumulation of errors, limiting practical applicability.
								
									<br><br>

									In this work we introduce the <i>thermalizer</i>, a modified diffusion model framework that can be used to stabilise autoregressive rollouts on the fly, extending the time-horizon of stable predictions by orders of magnitude.
									<br><br>
									<i>Project ongoing...</i>
								</div></p>
						</section>
						
						<hr />

						<section>
							<br>
							<header class="major">
								<h3 style="text-align:left">Deep learning models of subgrid dynamics</h3>
								<p><div align="left"><span class="image rightsmall"><img src="images/project_images/turbulence_wiki.jpg" alt=""/></span>
									Modelling the evolution of Earth's climate requires detailed simulations of the oceans and atmosphere. These simulations are extremely computationally expensive, and we are limited by computational resources in the small-scale dynamics that one can resolve. However, turbulence is a notoriously multi-scale phenomena, with small-scale eddies and vorticies affecting the large scale evolution of fluids. Therefore ocean simulations of finite resolution are missing these important effects.
									<br><br>
									In this project we develop coupled physics + deep learning models of turbulent flows, where the machine learning component models the unresolved, subgrid dynamics, enabling accurate, coarse resolution simulations. In particular, we demonstrate how the stability of these coupled models can be improved by augmenting the training procedure with a neural network emulator.
								
									<br><br>
									<i><a href="https://arxiv.org/abs/2307.13144">Published at ICML 2023: Synergy of Scientific and Machine Learning Modeling Workshop</a></i>
								</div></p>
						</section>
						
						<hr />

						<section>
							<header class="major">
								<h3 style="text-align:right">Cancer-net</h3>
								<p><div align="left"><span class="image leftsmall"><img src="images/project_images/graph.png" alt=""/></span>
									The cost of DNA sequencing has reduced by orders of magnitude over the past few years, enabling the possibility of modelling the progression of cancer tumors as a function of their genetic mutations as more data becomes available. However there are still a unique set of challenges, as this highly protected data is segregated into small groups, making it difficult to train models on long genetic sequences from only a few hundreds of patients.
									<br><br>
									By grouping genes into subgraphs using biological prior knowledge, and using sparse connections between these graphs, we are building a binary classification model of prostate cancer tumors, which will predict whether or not a tumor is metaststic based on it's genetic markers. The motivation behind the graph network is to enable increased generalisability, and the sparse connections reduce the number of parameters of the model, enabling training on the small datasets available.
									<br><br>
									<i><a href="https://arxiv.org/abs/2309.16645">Submitted to Nature Machine Intelligence</a></i>
								
								</div></p>
						</section>
					

						<hr />

						<section>
							<header class="major">
								<h3 style="text-align:left">Learnable wavelet neural networks for cosmological inference</h3>
								<p><div align="left"><span class="image rightsmall"><img src="images/project_images/wavelet.png" alt=""/></span>
								An important part of cosmology is the process of extracting information from the cosmological fields observed by experiments and telescopes. Traditionally, these fields are compressed into summary statistics (the two-point correlation function), and then compared with theory to obtain constraints on physical parameters. However it is known there is information lost in this process. Recent work has shown that convolutional neural networks (CNNs) are able to retain this information, while optimally marginalising over astrophysical nuisance parameters.
								<br><br>
								CNNs however require notoriously large amounts of data to train, and in the case of cosmological analysis, have to be trained on expensive numerical simulations. Given the limitations of computational resources, there is a strong motivation for designing models that can be trained on smaller amounts of training data. We designed a wavelet-based convolutional neural network, where the first 2 convolutional layers use wavelets instead of the traditional grid filters. The input fields are downsampled by factor of 2 after each convolution, meaning the wavelet layers are effectively performing an efficient compression of the data. The output of the wavelet convolutional layers are passed to a standard CNN of only 3 layers.
								<br><br>
								This model has an order of magnitude less learnable parameters than a standard CNN, and we show it dramatically outperforms a CNN at inference of cosmological parameters when in the regime of small training set sizes. Additionally, the wavelet filter parameters are included in the gradient descent, and can be used to understand where the information lies in the cosmological fields.
								<br><br>
								<i><a href="https://ml4astro.github.io/icml2022/assets/40.pdf">Published at ICML 2022 Machine learning for astronomy workshop</a></i>

						</section>


						<hr />

						<section>
								<header class="major">
									<h3 style="text-align:right">Compressing cosmological information</h3>
									<p><div align="left"><span class="image leftsmall"><img src="images/project_images/Lymanalpha.gif" alt=""/></span>The <a href="https://en.wikipedia.org/wiki/Lyman-alpha_forest">Lyman-alpha forest</a>, a series of absorption features in the spectra of distant quasars, can be used to measure the growth of structure in the early Universe, and on relatively small scales. This regime is particularly significant in constraints on two active areas of cosmological research. One is the effect of neutrino mass on cosmology, where subtle measurements of the growth of structure can be used to determine the mass of the lightest particle in physics. Another is the scale-dependence of the quantum fluctuations seeded at the Big Bang, which can be used to constrain models of fundamental physics.
									<br><br>
									Whilst this information is extremely valuable, analysis of the Lyman-alpha forest is a highly complicated task, requiring expensive hydrodynamical simulations, and marginalisation over uncertain astrophysical effects which the absorption features we observe also depend on. Only a small number of groups have the expertise and scope required to perform this analysis. As a result of these obstacles, results from the Lyman-alpha forest are underutilised in the joint analysis of cosmological observations. In this work, we tackle this problem by showing that the cosmological information in the Lyman-alpha forest can be compressed into marginalised constraints on just two parameters, in a model-independent way and with negligible loss of information. This will enable groups to include these valuable measurements in their analysis, without having to runn their own simulations or perform their own marginalisation, and will dramatically boost the scientific impact of current and future Lyman-alpha forest observations.
									<br><br>
									<i><a href="https://iopscience.iop.org/article/10.3847/1538-4357/acb433/meta">Published in the Astrophysical Journal</a></i>
						</section>

						<hr />

						<section>
								<header class="major">
									<h3 style="text-align:left">Gaussian process emulator</h3>
									<p><div align="left"><span class="image rightsmall"><img src="images/project_images/gaussprocess.png" alt=""/></span>The <a href="https://en.wikipedia.org/wiki/Lyman-alpha_forest">Lyman-alpha forest</a> is a valuble probe of the growth of structure in the early Universe, and on small scales. Interpeting observations of the Lyman-alpha forest requires running expensive hydrodynamical simulations, to compare different theoretical models to data. Modern statistical techniques require hundreds of thousands of likelihood evaluations in order to obtain robust statistical constraints on parameters, but we are limited by computational resources to just tens of simulations.

									In this paper, we tackle this problem by running a Latin hypercube of simulations, and use them to train a Gaussian process to predict the Lyman-alpha forest observables throughout parameter space. In particular, we use a physically motivated, lower-dimensional parameter space than previous work, and demonstrate the suitability of this parameter space by accurately predicting the Lyman-alpha forest for models not included in the original training data. This emulator is therefore appropriate for the analysis of several different interesting models simulatenously.
									<br><br>
									<i><a href="https://iopscience.iop.org/article/10.1088/1475-7516/2021/05/033">Published in JCAP</a></i><br>
									<i><a href="https://github.com/igmhub/LaCE/">Gaussian process code</a></i>
						</section>

						<hr />

						<section>
							<header class="major">
								<h3 style="text-align:right">Neutrino mass degeneracy</h3>
								<p><div align="left"><span class="image leftsmall"><img src="images/project_images/neutrino.jpg" alt=""/></span>One of the most significant contributions of the Lyman-alpha forest to cosmology is it's sensitity to the clustering of matter on small scales. This is particularly relevant in the case of neutrino mass, which causes a scale-dependent suppression of the growth of structure. In this work we run a set of hydrodynamical simulations with the intention to carefully understand the effects of neutrino mass on the forest. Specifically, we want to understand to what extent the effects of neutrino mass are degenerate with an overall rescaling of the amplitude of clustering. The motivation is that it is important to understand parameter degeneracies when designing methods to interpret a given dataset.
								<br><br>
								We find that the Lyman-alpha forest alone is unable to distinguish between the effects of neutrino mass and a uniform rescaling of the clustering amplitude. This has strong implications for the design of future modelling frameworks and likelihoods.
								<br><br>
								<i><a href="https://iopscience.iop.org/article/10.1088/1475-7516/2020/04/025">Published in JCAP</a></i><br>
						</section>

						<hr />

						<section>
							<header class="major">
								<h3 style="text-align:left">Precessing binary black holes</h3>
								<p><div align="left"><span class="image rightsmall"><img src="images/project_images/bbh.png" alt=""/></span>In 2016, LIGO made the first ever detection of gravitational waves, from a system of two orbiting black holes colliding and merging into one. This opened up a new way to observe the Universe. Recovering the properties of the progenitor black holes of a merger event is crucial in maximising the science that can be done with gravitational wave astronomy. One property that is particularly relevant for understanding the formation mechanism of these binary systems is the spin of the black holes. In systems where the spins of the component black holes are not aligned, the orbital plane of the binary precesses, like a plate or a coin wobbles when dropped on a flat surface.
								<br><br>
								In this project I ran simulated mock merger events, and used Bayesian inference and Markov chain Monte Carlo analysis in order to understand our ability to observe this precession. I found that this strongly depends on the inclination of the binary system with respect to Earth, as well as it's location in the sky, due to parameter degeneracies.
								<br><br>
								<i><a href="things/mthesis.pdf">Masters thesis</a> - seed project for a publication in <a href="https://journals.aps.org/prd/abstract/10.1103/PhysRevD.103.124023">Physical Review  D</a></i><br><br>
						</section>
					</section>
				</section>
			</div>

		<!-- Three -->
			<section id="skills" class="wrapper style1">
				<div class="container">
					<header class="major">
						<h3>Technical skills</h3>
						</header>
						Below is a summary of the mathematical and computational techniques I have employed over the course of my research career<br><br>
						<div class="row">
							<div class="4u 6u(2) 12u$(3)">
								<b>Mathematical</b>
								<ul>
									<li>Linear algebra</li>
									<li>Probability & statistics</li>
									<li>Calculus</li>
									<li>Partial differential equations</li>
								</ul>
							</div>
							<div class="4u 6u$(2) 12u$(3)">
								<b>Computational</b>
								<ul>
									<li>Python</li>
									<li>C++</li>
									<li>Linux/bash</li>
									<li>High performance/parallel computing</li>
									<li>git/github</li>
									<li>PyTorch, TensorFlow, Jax</li>
								</ul>
							</div>
						    <div class="4u 6u$(2) 12u$(3)">
								<b>Statistical/ML</b>
								<ul>
									<li>Bayesian Inference</li>
									<li>Markov chain Monte Carlo, Metropolis Hastings sampling</li>
									<li>Gaussian processes</li>
									<li>Linear regression, logistic regression</li>
									<li>Convolutional neural networks, vision transformers</li>
									<li>Generative modelling: VAE, diffusion models</li>
									<li>Graph neural networks/message passing networks</li>
									<li>Scattering transform</li>
								</ul>
							</div>
						</div>

				</div>
			</section>
			
		<!-- Footer -->
			<footer id="footer">
				<ul class="menu">
					<li><a href="contact.html">Contact</a></li>
				</ul>
			</footer>

	</body>
</html>
